"""Multi-AI Router â€” Claude + GPT + Gemini ë¶„ì—… ì‹œìŠ¤í…œ.

K-Quant v3.6: ê° AIì˜ ê°•ì ì— ë§ëŠ” íƒœìŠ¤í¬ ë¶„ë°°.
- Claude (Anthropic): ì‹¬ì¸µ ë¶„ì„, OCR/Vision, ì „ëµ í•©ì„±
- GPT (OpenAI): ê¸°ìˆ ì  ë¶„ì„, êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶œë ¥
- Gemini (Google): ë‰´ìŠ¤ ê°ì„±ë¶„ì„, ë¹ ë¥¸ ìš”ì•½, í•œêµ­ì–´ ì†ë„

Usage:
    router = AIRouter()
    result = await router.analyze("sentiment", prompt, context)
"""

from __future__ import annotations

import json
import logging
import os
import time
from dataclasses import dataclass, field
from typing import Any

import httpx

logger = logging.getLogger(__name__)


# â”€â”€ AI Provider Configs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@dataclass
class AIUsageStats:
    """API í˜¸ì¶œ í†µê³„ ì¶”ì ."""
    calls: int = 0
    tokens_in: int = 0
    tokens_out: int = 0
    errors: int = 0
    total_latency_ms: float = 0.0

    @property
    def avg_latency_ms(self) -> float:
        return self.total_latency_ms / self.calls if self.calls > 0 else 0


@dataclass
class ProviderConfig:
    name: str
    api_key: str
    models: dict[str, str] = field(default_factory=dict)
    available: bool = False


# â”€â”€ Task-to-AI ë¼ìš°íŒ… í…Œì´ë¸” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

TASK_ROUTING: dict[str, dict[str, Any]] = {
    # â”€ Gemini ë‹´ë‹¹: ì†ë„ ì¤‘ì‹œ, ë¹„ìš© ì ˆì•½ â”€
    "sentiment": {
        "provider": "gemini",
        "model_tier": "fast",
        "fallback": "claude",
        "description": "ë‰´ìŠ¤/ê°ì„± ë¶„ì„ (ë¹ ë¥¸ í•œêµ­ì–´ ì²˜ë¦¬)",
    },
    "news_summary": {
        "provider": "gemini",
        "model_tier": "fast",
        "fallback": "claude",
        "description": "ë‰´ìŠ¤ ìš”ì•½ (ì†ë„ ìš°ì„ )",
    },
    "morning_briefing": {
        "provider": "gemini",
        "model_tier": "fast",
        "fallback": "claude",
        "description": "ëª¨ë‹ ë¸Œë¦¬í•‘ (ë¹ ë¥¸ ìƒì„±)",
    },
    "live_market": {
        "provider": "gemini",
        "model_tier": "fast",
        "fallback": "claude",
        "description": "ì‹¤ì‹œê°„ ì‹œí™© ìš”ì•½",
    },
    # â”€ GPT ë‹´ë‹¹: êµ¬ì¡°í™”ëœ ì¶œë ¥, ê¸°ìˆ  ë¶„ì„ â”€
    "technical_analysis": {
        "provider": "gpt",
        "model_tier": "fast",
        "fallback": "claude",
        "description": "ê¸°ìˆ ì  ë¶„ì„ (êµ¬ì¡°í™”ëœ JSON ì¶œë ¥)",
    },
    "fundamental_analysis": {
        "provider": "gpt",
        "model_tier": "fast",
        "fallback": "claude",
        "description": "í€ë”ë©˜íƒˆ ë¶„ì„ (ë°ì´í„° í•´ì„)",
    },
    "diagnosis_batch": {
        "provider": "gpt",
        "model_tier": "fast",
        "fallback": "claude",
        "description": "ë³´ìœ ì¢…ëª© ì¼ê´„ ì§„ë‹¨",
    },
    "sector_analysis": {
        "provider": "gpt",
        "model_tier": "standard",
        "fallback": "claude",
        "description": "ì„¹í„° ë¶„ì„ ë¦¬í¬íŠ¸",
    },
    # â”€ Claude ë‹´ë‹¹: ì‹¬ì¸µ ë¶„ì„, Vision, ì „ëµ â”€
    "deep_analysis": {
        "provider": "claude",
        "model_tier": "standard",
        "fallback": "gpt",
        "description": "ì‹¬ì¸µ ì¢…í•© ë¶„ì„",
    },
    "strategy_synthesis": {
        "provider": "claude",
        "model_tier": "standard",
        "fallback": "gpt",
        "description": "ì „ëµ í•©ì„± (ë©€í‹°ì—ì´ì „íŠ¸ ìµœì¢…)",
    },
    "vision_ocr": {
        "provider": "claude",
        "model_tier": "standard",
        "fallback": None,
        "description": "ìŠ¤í¬ë¦°ìƒ·/ì´ë¯¸ì§€ ë¶„ì„ (Vision)",
    },
    "chat": {
        "provider": "claude",
        "model_tier": "standard",
        "fallback": "gpt",
        "description": "ëŒ€í™”í˜• AI ì§ˆì˜",
    },
    "pdf_report": {
        "provider": "claude",
        "model_tier": "standard",
        "fallback": "gpt",
        "description": "í”„ë¦¬ë¯¸ì—„ PDF ë¦¬í¬íŠ¸ ìƒì„±",
    },
    "eod_report": {
        "provider": "claude",
        "model_tier": "standard",
        "fallback": "gpt",
        "description": "ì¥ ë§ˆê° ë¶„ì„ ë¦¬í¬íŠ¸",
    },
    # â”€ v3.10 ì¶”ê°€ â”€
    "us_premarket": {
        "provider": "gpt",
        "model_tier": "standard",
        "fallback": "claude",
        "description": "ë¯¸êµ­ í”„ë¦¬ë§ˆì¼“ ë¸Œë¦¬í•‘",
    },
}


class AIRouter:
    """Multi-AI ë¼ìš°í„°: íƒœìŠ¤í¬ë³„ ìµœì  AI ìë™ ì„ íƒ."""

    def __init__(self) -> None:
        self.providers: dict[str, ProviderConfig] = {}
        self.stats: dict[str, AIUsageStats] = {
            "claude": AIUsageStats(),
            "gpt": AIUsageStats(),
            "gemini": AIUsageStats(),
        }
        # [v3.6.4] Prompt Caching í†µê³„
        self._cache_hits: int = 0
        self._cache_tokens_saved: int = 0
        self._init_providers()

    def _init_providers(self) -> None:
        """í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ ë¡œë“œ + í”„ë¡œë°”ì´ë” ì´ˆê¸°í™”."""
        # Claude (Anthropic)
        claude_key = os.getenv("ANTHROPIC_API_KEY", "")
        self.providers["claude"] = ProviderConfig(
            name="Claude (Anthropic)",
            api_key=claude_key,
            models={
                "fast": "claude-haiku-4-5-20251001",
                "standard": "claude-sonnet-4-5-20250929",
                "vision": "claude-sonnet-4-20250514",
            },
            available=bool(claude_key),
        )

        # GPT (OpenAI)
        gpt_key = os.getenv("OPENAI_API_KEY", "")
        self.providers["gpt"] = ProviderConfig(
            name="GPT (OpenAI)",
            api_key=gpt_key,
            models={
                "fast": "gpt-4o-mini",
                "standard": "gpt-4o",
            },
            available=bool(gpt_key),
        )

        # Gemini (Google)
        gemini_key = os.getenv("GEMINI_API_KEY", "")
        self.providers["gemini"] = ProviderConfig(
            name="Gemini (Google)",
            api_key=gemini_key,
            models={
                "fast": "gemini-2.0-flash",
                "standard": "gemini-2.0-pro",
            },
            available=bool(gemini_key),
        )

        available = [n for n, p in self.providers.items() if p.available]
        logger.info("AI Router initialized: %s available", ", ".join(available) or "NONE")

    # â”€â”€ Core API â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def analyze(
        self,
        task: str,
        prompt: str,
        *,
        system: str = "",
        max_tokens: int = 1000,
        temperature: float = 0.3,
        response_format: str = "text",  # "text" or "json"
    ) -> str:
        """íƒœìŠ¤í¬ì— ë§ëŠ” AIë¡œ ë¶„ì„ ì‹¤í–‰.

        Args:
            task: TASK_ROUTING í‚¤ (sentiment, technical_analysis, etc.)
            prompt: ì‚¬ìš©ì/ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            system: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            max_tokens: ìµœëŒ€ í† í°
            temperature: ì°½ì˜ì„± (0~1)
            response_format: ì‘ë‹µ í˜•ì‹

        Returns:
            AI ì‘ë‹µ í…ìŠ¤íŠ¸
        """
        routing = TASK_ROUTING.get(task)
        if not routing:
            logger.warning("Unknown task '%s', falling back to claude", task)
            routing = {"provider": "claude", "model_tier": "standard", "fallback": "gpt"}

        provider_name = routing["provider"]
        model_tier = routing["model_tier"]
        fallback = routing.get("fallback")

        # 1ì°¨: ì§€ì •ëœ í”„ë¡œë°”ì´ë”
        provider = self.providers.get(provider_name)
        if provider and provider.available:
            try:
                result = await self._call_provider(
                    provider_name, model_tier, prompt,
                    system=system, max_tokens=max_tokens,
                    temperature=temperature, response_format=response_format,
                )
                if result:
                    return result
            except Exception as e:
                logger.warning("AI %s failed for task '%s': %s", provider_name, task, e)
                self.stats[provider_name].errors += 1

        # 2ì°¨: Fallback
        if fallback:
            fb_provider = self.providers.get(fallback)
            if fb_provider and fb_provider.available:
                try:
                    logger.info("Falling back to %s for task '%s'", fallback, task)
                    result = await self._call_provider(
                        fallback, model_tier, prompt,
                        system=system, max_tokens=max_tokens,
                        temperature=temperature, response_format=response_format,
                    )
                    if result:
                        return result
                except Exception as e:
                    logger.warning("Fallback %s also failed: %s", fallback, e)
                    self.stats[fallback].errors += 1

        # 3ì°¨: ì•„ë¬´ ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡œë°”ì´ë”
        for name, prov in self.providers.items():
            if prov.available and name not in (provider_name, fallback):
                try:
                    return await self._call_provider(
                        name, model_tier, prompt,
                        system=system, max_tokens=max_tokens,
                        temperature=temperature, response_format=response_format,
                    )
                except Exception as e:
                    logger.warning("AI %s last-resort fallback failed for task: %s", name, e)
                    continue

        return "[AI ì‘ë‹µ ë¶ˆê°€] ëª¨ë“  AI í”„ë¡œë°”ì´ë”ê°€ ì‚¬ìš© ë¶ˆê°€í•©ë‹ˆë‹¤."

    # â”€â”€ Provider-specific Calls â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def _call_provider(
        self,
        provider_name: str,
        model_tier: str,
        prompt: str,
        *,
        system: str = "",
        max_tokens: int = 1000,
        temperature: float = 0.3,
        response_format: str = "text",
    ) -> str:
        """í”„ë¡œë°”ì´ë”ë³„ API í˜¸ì¶œ."""
        provider = self.providers[provider_name]
        model = provider.models.get(model_tier, list(provider.models.values())[0])

        start = time.monotonic()
        try:
            if provider_name == "claude":
                result = await self._call_claude(
                    provider.api_key, model, prompt,
                    system=system, max_tokens=max_tokens, temperature=temperature,
                )
            elif provider_name == "gpt":
                result = await self._call_gpt(
                    provider.api_key, model, prompt,
                    system=system, max_tokens=max_tokens, temperature=temperature,
                    response_format=response_format,
                )
            elif provider_name == "gemini":
                result = await self._call_gemini(
                    provider.api_key, model, prompt,
                    system=system, max_tokens=max_tokens, temperature=temperature,
                )
            else:
                return ""

            elapsed = (time.monotonic() - start) * 1000
            stats = self.stats[provider_name]
            stats.calls += 1
            stats.total_latency_ms += elapsed
            logger.debug(
                "AI %s/%s responded in %.0fms (%d chars)",
                provider_name, model, elapsed, len(result),
            )
            return result
        except Exception:
            elapsed = (time.monotonic() - start) * 1000
            self.stats[provider_name].total_latency_ms += elapsed
            raise

    async def _call_claude(
        self, api_key: str, model: str, prompt: str, *,
        system: str = "", max_tokens: int = 1000, temperature: float = 0.3,
    ) -> str:
        """Anthropic Claude API í˜¸ì¶œ (Prompt Caching ì ìš©)."""
        payload: dict[str, Any] = {
            "model": model,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "messages": [{"role": "user", "content": prompt}],
        }
        # [v3.6.4] Prompt Caching: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ìºì‹œí•˜ì—¬ ë¹„ìš© 90% ì ˆê°
        if system:
            payload["system"] = [{
                "type": "text",
                "text": system,
                "cache_control": {"type": "ephemeral"},
            }]

        async with httpx.AsyncClient(timeout=60) as client:
            resp = await client.post(
                "https://api.anthropic.com/v1/messages",
                headers={
                    "x-api-key": api_key,
                    "anthropic-version": "2023-06-01",
                    "content-type": "application/json",
                },
                json=payload,
            )
            if resp.status_code != 200:
                raise RuntimeError(f"Claude API error {resp.status_code}: {resp.text[:200]}")
            data = resp.json()
            usage = data.get("usage", {})
            self.stats["claude"].tokens_in += usage.get("input_tokens", 0)
            self.stats["claude"].tokens_out += usage.get("output_tokens", 0)
            # ìºì‹œ íˆíŠ¸ í†µê³„ ì¶”ì 
            cache_read = usage.get("cache_read_input_tokens", 0)
            cache_write = usage.get("cache_creation_input_tokens", 0)
            if cache_read > 0:
                self._cache_hits += 1
                self._cache_tokens_saved += cache_read
            return data["content"][0]["text"]

    async def _call_gpt(
        self, api_key: str, model: str, prompt: str, *,
        system: str = "", max_tokens: int = 1000, temperature: float = 0.3,
        response_format: str = "text",
    ) -> str:
        """OpenAI GPT API í˜¸ì¶œ."""
        messages = []
        if system:
            messages.append({"role": "system", "content": system})
        messages.append({"role": "user", "content": prompt})

        payload: dict[str, Any] = {
            "model": model,
            "messages": messages,
            "max_tokens": max_tokens,
            "temperature": temperature,
        }
        if response_format == "json":
            payload["response_format"] = {"type": "json_object"}

        async with httpx.AsyncClient(timeout=60) as client:
            resp = await client.post(
                "https://api.openai.com/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {api_key}",
                    "Content-Type": "application/json",
                },
                json=payload,
            )
            if resp.status_code != 200:
                raise RuntimeError(f"GPT API error {resp.status_code}: {resp.text[:200]}")
            data = resp.json()
            usage = data.get("usage", {})
            self.stats["gpt"].tokens_in += usage.get("prompt_tokens", 0)
            self.stats["gpt"].tokens_out += usage.get("completion_tokens", 0)
            return data["choices"][0]["message"]["content"]

    async def _call_gemini(
        self, api_key: str, model: str, prompt: str, *,
        system: str = "", max_tokens: int = 1000, temperature: float = 0.3,
    ) -> str:
        """Google Gemini API í˜¸ì¶œ."""
        contents = []
        if system:
            contents.append({"role": "user", "parts": [{"text": f"[System Instructions]\n{system}"}]})
            contents.append({"role": "model", "parts": [{"text": "Understood. I will follow these instructions."}]})
        contents.append({"role": "user", "parts": [{"text": prompt}]})

        payload = {
            "contents": contents,
            "generationConfig": {
                "maxOutputTokens": max_tokens,
                "temperature": temperature,
            },
        }

        url = (
            f"https://generativelanguage.googleapis.com/v1beta/models/{model}"
            f":generateContent?key={api_key}"
        )

        async with httpx.AsyncClient(timeout=60) as client:
            resp = await client.post(url, json=payload)
            if resp.status_code != 200:
                raise RuntimeError(f"Gemini API error {resp.status_code}: {resp.text[:200]}")
            data = resp.json()
            # Gemini usage tracking
            usage = data.get("usageMetadata", {})
            self.stats["gemini"].tokens_in += usage.get("promptTokenCount", 0)
            self.stats["gemini"].tokens_out += usage.get("candidatesTokenCount", 0)
            candidates = data.get("candidates", [])
            if candidates:
                parts = candidates[0].get("content", {}).get("parts", [])
                if parts:
                    return parts[0].get("text", "")
            return ""

    # â”€â”€ Vision (Claude Only) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def vision_analyze(
        self, image_bytes: bytes, prompt: str, *, max_tokens: int = 4096,
    ) -> str:
        """ì´ë¯¸ì§€ ë¶„ì„ (Claude Vision ì „ìš©)."""
        import base64

        provider = self.providers.get("claude")
        if not provider or not provider.available:
            return "[Vision ë¶ˆê°€] Claude API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

        image_b64 = base64.standard_b64encode(image_bytes).decode("ascii")

        # Detect media type
        if image_bytes[:8] == b"\x89PNG\r\n\x1a\n":
            media_type = "image/png"
        elif image_bytes[:2] == b"\xff\xd8":
            media_type = "image/jpeg"
        else:
            media_type = "image/jpeg"

        payload = {
            "model": provider.models.get("vision", provider.models["standard"]),
            "max_tokens": max_tokens,
            "messages": [{
                "role": "user",
                "content": [
                    {"type": "image", "source": {
                        "type": "base64", "media_type": media_type, "data": image_b64,
                    }},
                    {"type": "text", "text": prompt},
                ],
            }],
        }

        start = time.monotonic()
        async with httpx.AsyncClient(timeout=60) as client:
            resp = await client.post(
                "https://api.anthropic.com/v1/messages",
                headers={
                    "x-api-key": provider.api_key,
                    "anthropic-version": "2023-06-01",
                    "content-type": "application/json",
                },
                json=payload,
            )
            elapsed = (time.monotonic() - start) * 1000
            self.stats["claude"].calls += 1
            self.stats["claude"].total_latency_ms += elapsed

            if resp.status_code != 200:
                raise RuntimeError(f"Vision API error: {resp.status_code}")
            data = resp.json()
            self.stats["claude"].tokens_in += data.get("usage", {}).get("input_tokens", 0)
            self.stats["claude"].tokens_out += data.get("usage", {}).get("output_tokens", 0)
            return data["content"][0]["text"]

    # â”€â”€ Status & Monitoring â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def get_status(self) -> str:
        """AI í”„ë¡œë°”ì´ë” ìƒíƒœ ìš”ì•½."""
        lines = ["AI ì—”ì§„ ìƒíƒœ", "â”€" * 25]
        for name, provider in self.providers.items():
            status = "âœ…" if provider.available else "âŒ"
            stats = self.stats[name]
            model_count = len(provider.models)
            line = f"{status} {provider.name}: {model_count}ëª¨ë¸"
            if stats.calls > 0:
                line += f" | {stats.calls}íšŒ í˜¸ì¶œ | í‰ê·  {stats.avg_latency_ms:.0f}ms"
                if stats.errors > 0:
                    line += f" | âš ï¸ {stats.errors}ì˜¤ë¥˜"
            lines.append(line)

        # ë¹„ìš© ì¶”ì • (ëŒ€ëµ)
        claude_cost = (
            self.stats["claude"].tokens_in * 0.25 / 1_000_000
            + self.stats["claude"].tokens_out * 1.25 / 1_000_000
        )
        gpt_cost = (
            self.stats["gpt"].tokens_in * 0.15 / 1_000_000
            + self.stats["gpt"].tokens_out * 0.60 / 1_000_000
        )
        gemini_cost = (
            self.stats["gemini"].tokens_in * 0.075 / 1_000_000
            + self.stats["gemini"].tokens_out * 0.30 / 1_000_000
        )
        total = claude_cost + gpt_cost + gemini_cost
        lines.append("")
        lines.append(f"ğŸ’° ì„¸ì…˜ ë¹„ìš©: ~${total:.4f}")
        lines.append(f"   Claude: ${claude_cost:.4f} | GPT: ${gpt_cost:.4f} | Gemini: ${gemini_cost:.4f}")

        # ìºì‹œ ì ˆê° í†µê³„
        if self._cache_hits > 0:
            saved_cost = self._cache_tokens_saved * 2.70 / 1_000_000  # $3 - $0.30 = $2.70 saved per MTok
            lines.append(f"\nğŸ—„ ìºì‹œ íˆíŠ¸: {self._cache_hits}íšŒ | ì ˆê° í† í°: {self._cache_tokens_saved:,}")
            lines.append(f"   ì˜ˆìƒ ì ˆê°: ~${saved_cost:.4f}")

        return "\n".join(lines)

    def get_routing_table(self) -> str:
        """í˜„ì¬ ë¼ìš°íŒ… í…Œì´ë¸” í‘œì‹œ."""
        lines = ["AI ì—…ë¬´ ë¶„ë‹´í‘œ", "â•" * 30]
        by_provider: dict[str, list[str]] = {"gemini": [], "gpt": [], "claude": []}
        for task, config in TASK_ROUTING.items():
            provider = config["provider"]
            if provider in by_provider:
                by_provider[provider].append(f"  â€¢ {config['description']}")

        icons = {"gemini": "ğŸŸ¢ Gemini", "gpt": "ğŸ”µ GPT", "claude": "ğŸŸ£ Claude"}
        for prov, tasks in by_provider.items():
            available = self.providers.get(prov, ProviderConfig("", "")).available
            status = "âœ…" if available else "âŒ"
            lines.append(f"\n{icons[prov]} {status}")
            lines.extend(tasks)

        return "\n".join(lines)
