"""Point-in-Time ë°ì´í„° ë¬´ê²°ì„± ì—”ì§„ â€” v5.0-1.

ëª¨ë“  ë°ì´í„°ì— ì‹œì (event_time), ìˆ˜ì§‘ì‹œê°„(ingest_time), ì¶œì²˜(source)ë¥¼
íƒœê¹…í•˜ì—¬ ë¯¸ë˜ ë°ì´í„° ìœ ì…(look-ahead bias)ì„ ì›ì²œ ì°¨ë‹¨í•œë‹¤.

í•µì‹¬ ê¸°ëŠ¥:
  1. DataPoint ë˜í¼ â€” ëª¨ë“  ìˆ˜ì¹˜ ë°ì´í„°ì— ì‹œì Â·ì¶œì²˜ ë©”íƒ€ ì²¨ë¶€
  2. AsOfJoin â€” íŠ¹ì • ê¸°ì¤€ ì‹œì (cutoff) ì´ì „ ë°ì´í„°ë§Œ ë°˜í™˜
  3. SourceRegistry â€” ë°ì´í„° ì¶œì²˜ë³„ ì§€ì—°(latency) ê´€ë¦¬
  4. PITValidator â€” ë°ì´í„° í”„ë ˆì„ ì „ì²´ì˜ ì‹œì  ë¬´ê²°ì„± ê²€ì¦
"""

from __future__ import annotations

import logging
import time
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from typing import Any

import pandas as pd

from kstock.core.tz import KST

logger = logging.getLogger(__name__)


# â”€â”€ ë°ì´í„° ì†ŒìŠ¤ ì—´ê±° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class DataSourceType(str, Enum):
    """ë°ì´í„° ì¶œì²˜ ìœ í˜•."""
    KIS_REALTIME = "kis_realtime"     # KIS ì‹¤ì‹œê°„ (ì§€ì—° ~0.5ì´ˆ)
    KIS_HISTORICAL = "kis_historical"  # KIS ì¼ë´‰ (T+0 15:30 í™•ì •)
    YFINANCE = "yfinance"              # yfinance (ì§€ì—° ~15ë¶„)
    NAVER = "naver"                    # ë„¤ì´ë²„ ê¸ˆìœµ (ì§€ì—° ~20ë¶„)
    MANUAL = "manual"                  # ìˆ˜ë™ ì…ë ¥
    SCREENSHOT = "screenshot"          # ìŠ¤í¬ë¦°ìƒ· OCR
    BACKTEST = "backtest"              # ë°±í…ŒìŠ¤íŠ¸ í•©ì„± ë°ì´í„°


# ì†ŒìŠ¤ë³„ ì¶”ì • ì§€ì—° ì‹œê°„ (ì´ˆ)
SOURCE_LATENCY: dict[str, float] = {
    DataSourceType.KIS_REALTIME: 0.5,
    DataSourceType.KIS_HISTORICAL: 0.0,    # í™•ì • ë°ì´í„°
    DataSourceType.YFINANCE: 900.0,        # ~15ë¶„
    DataSourceType.NAVER: 1200.0,          # ~20ë¶„
    DataSourceType.MANUAL: 0.0,
    DataSourceType.SCREENSHOT: 0.0,
    DataSourceType.BACKTEST: 0.0,
}


# â”€â”€ ë°ì´í„° í¬ì¸íŠ¸ ë˜í¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@dataclass
class DataPoint:
    """ì‹œì  íƒœê¹…ëœ ë‹¨ì¼ ë°ì´í„° í¬ì¸íŠ¸."""

    value: Any
    event_time: datetime          # ì´ë²¤íŠ¸ ë°œìƒ ì‹œì  (ì‹œì„¸ ê¸°ì¤€ ì‹œê°)
    ingest_time: datetime         # ì‹œìŠ¤í…œ ìˆ˜ì§‘ ì‹œì 
    source: str                   # DataSourceType ê°’
    ticker: str = ""
    field_name: str = ""          # "close", "volume", "rsi", etc.
    confidence: float = 1.0       # 0~1, ìˆ˜ë™ì…ë ¥ì€ ë‚®ê²Œ
    metadata: dict = field(default_factory=dict)

    @property
    def latency_seconds(self) -> float:
        """ìˆ˜ì§‘ ì§€ì—° ì‹œê°„ (ì´ˆ)."""
        return (self.ingest_time - self.event_time).total_seconds()

    @property
    def is_stale(self) -> bool:
        """ë°ì´í„°ê°€ 5ë¶„ ì´ìƒ ì§€ì—°ë˜ë©´ stale."""
        return self.latency_seconds > 300

    def to_dict(self) -> dict:
        """ì§ë ¬í™”."""
        return {
            "value": self.value,
            "event_time": self.event_time.isoformat(),
            "ingest_time": self.ingest_time.isoformat(),
            "source": self.source,
            "ticker": self.ticker,
            "field_name": self.field_name,
            "confidence": self.confidence,
            "latency_s": round(self.latency_seconds, 2),
        }

    @classmethod
    def now(cls, value: Any, source: str, ticker: str = "",
            field_name: str = "", **kwargs) -> DataPoint:
        """í˜„ì¬ ì‹œì ìœ¼ë¡œ ì¦‰ì‹œ ìƒì„±."""
        now = datetime.now(KST)
        latency = SOURCE_LATENCY.get(source, 0.0)
        event_time = now - timedelta(seconds=latency)
        return cls(
            value=value,
            event_time=event_time,
            ingest_time=now,
            source=source,
            ticker=ticker,
            field_name=field_name,
            **kwargs,
        )


# â”€â”€ As-Of Join ì—”ì§„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class AsOfJoinEngine:
    """ê¸°ì¤€ ì‹œì (cutoff) ì´ì „ ë°ì´í„°ë§Œ í—ˆìš©í•˜ëŠ” í•„í„°.

    ë°±í…ŒìŠ¤íŠ¸ì™€ ì‹¤ì „ ëª¨ë‘ì—ì„œ ë¯¸ë˜ ë°ì´í„° ìœ ì…ì„ ì°¨ë‹¨í•œë‹¤.

    Usage:
        engine = AsOfJoinEngine()
        # OHLCVì— ì‹œì  ì»¬ëŸ¼ ì¶”ê°€
        df = engine.tag_dataframe(df, source="yfinance", ticker="005930")
        # cutoff ì´ì „ ë°ì´í„°ë§Œ ì¶”ì¶œ
        safe = engine.filter_asof(df, cutoff=some_datetime)
    """

    @staticmethod
    def tag_dataframe(
        df: pd.DataFrame,
        source: str,
        ticker: str = "",
        event_col: str = "date",
    ) -> pd.DataFrame:
        """DataFrameì— ì‹œì Â·ì¶œì²˜ ë©”íƒ€ ì»¬ëŸ¼ì„ ì¶”ê°€í•œë‹¤.

        Args:
            df: OHLCV ë˜ëŠ” feature DataFrame.
            source: DataSourceType ê°’.
            ticker: ì¢…ëª© ì½”ë“œ.
            event_col: ì´ë²¤íŠ¸ ì‹œì  ì»¬ëŸ¼ëª… (ì—†ìœ¼ë©´ ì¸ë±ìŠ¤).

        Returns:
            _pit_event_time, _pit_ingest_time, _pit_source ì»¬ëŸ¼ì´ ì¶”ê°€ëœ DataFrame.
        """
        if df.empty:
            return df

        result = df.copy()
        now = datetime.now(KST)

        # event_time ê²°ì •
        if event_col in result.columns:
            result["_pit_event_time"] = pd.to_datetime(result[event_col])
        elif isinstance(result.index, pd.DatetimeIndex):
            result["_pit_event_time"] = result.index
        else:
            result["_pit_event_time"] = now

        result["_pit_ingest_time"] = now
        result["_pit_source"] = source
        result["_pit_ticker"] = ticker

        return result

    @staticmethod
    def filter_asof(
        df: pd.DataFrame,
        cutoff: datetime,
        event_col: str = "_pit_event_time",
    ) -> pd.DataFrame:
        """cutoff ì‹œì  ì´ì „ ë°ì´í„°ë§Œ ë°˜í™˜í•œë‹¤.

        Args:
            df: tag_dataframe()ìœ¼ë¡œ íƒœê¹…ëœ DataFrame.
            cutoff: ê¸°ì¤€ ì‹œì .

        Returns:
            cutoff ì´ì „ í–‰ë§Œ í¬í•¨í•˜ëŠ” DataFrame.
        """
        if df.empty or event_col not in df.columns:
            return df

        cutoff_ts = pd.Timestamp(cutoff)
        mask = df[event_col] <= cutoff_ts
        filtered = df.loc[mask]

        dropped = len(df) - len(filtered)
        if dropped > 0:
            logger.info(
                "AsOfJoin: cutoff=%s ê¸°ì¤€ %dí–‰ ì œê±° (ë¯¸ë˜ ë°ì´í„° ì°¨ë‹¨)",
                cutoff.isoformat(), dropped,
            )

        return filtered

    @staticmethod
    def latest_asof(
        df: pd.DataFrame,
        cutoff: datetime,
        event_col: str = "_pit_event_time",
    ) -> pd.Series | None:
        """cutoff ì´ì „ ê°€ì¥ ìµœì‹  í–‰ì„ ë°˜í™˜í•œë‹¤.

        Args:
            df: íƒœê¹…ëœ DataFrame.
            cutoff: ê¸°ì¤€ ì‹œì .

        Returns:
            ìµœì‹  í–‰ (Series) ë˜ëŠ” None.
        """
        filtered = AsOfJoinEngine.filter_asof(df, cutoff, event_col)
        if filtered.empty:
            return None
        return filtered.iloc[-1]


# â”€â”€ PIT Validator â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@dataclass
class PITViolation:
    """ì‹œì  ë¬´ê²°ì„± ìœ„ë°˜."""
    violation_type: str    # "future_data", "stale_data", "source_mismatch"
    severity: str          # "critical", "warning"
    description: str
    row_index: int | None = None
    details: dict = field(default_factory=dict)


class PITValidator:
    """ë°ì´í„° í”„ë ˆì„ì˜ ì‹œì  ë¬´ê²°ì„±ì„ ê²€ì¦í•œë‹¤.

    ì£¼ìš” ê²€ì¦:
      1. ë¯¸ë˜ ë°ì´í„° (event_time > í˜„ì¬ì‹œê°„): critical
      2. ì‹œì  ì—­ì „ (event_timeì´ ì •ë ¬ë˜ì§€ ì•ŠìŒ): warning
      3. ìŠ¤í…Œì¼ ë°ì´í„° (ingest - event > threshold): warning
      4. ì†ŒìŠ¤ ì¼ê´€ì„± (í•œ ë°ì´í„°ì…‹ì— ì—¬ëŸ¬ ì†ŒìŠ¤ í˜¼í•©): warning
    """

    def __init__(self, stale_threshold_seconds: float = 600.0):
        self.stale_threshold = stale_threshold_seconds

    def validate(
        self,
        df: pd.DataFrame,
        reference_time: datetime | None = None,
    ) -> list[PITViolation]:
        """DataFrame ì „ì²´ì˜ ì‹œì  ë¬´ê²°ì„±ì„ ê²€ì¦í•œë‹¤.

        Args:
            df: _pit_event_time, _pit_ingest_time, _pit_source ì»¬ëŸ¼ í•„ìš”.
            reference_time: ê¸°ì¤€ ì‹œì . Noneì´ë©´ í˜„ì¬ ì‹œê°.

        Returns:
            ìœ„ë°˜ ë¦¬ìŠ¤íŠ¸.
        """
        violations: list[PITViolation] = []

        if df.empty:
            return violations

        ref = reference_time or datetime.now(KST)
        ref_ts = pd.Timestamp(ref)

        # 1. ë¯¸ë˜ ë°ì´í„° ê²€ì‚¬
        if "_pit_event_time" in df.columns:
            future_mask = df["_pit_event_time"] > ref_ts
            future_count = future_mask.sum()
            if future_count > 0:
                violations.append(PITViolation(
                    violation_type="future_data",
                    severity="critical",
                    description=(
                        f"ë¯¸ë˜ ë°ì´í„° {future_count}í–‰ ë°œê²¬. "
                        "Look-ahead bias ìœ„í—˜."
                    ),
                    details={"count": int(future_count)},
                ))

            # 2. ì‹œì  ì—­ì „ ê²€ì‚¬
            event_times = df["_pit_event_time"]
            if not event_times.is_monotonic_increasing:
                reversals = (event_times.diff() < pd.Timedelta(0)).sum()
                if reversals > 0:
                    violations.append(PITViolation(
                        violation_type="time_reversal",
                        severity="warning",
                        description=(
                            f"ì‹œì  ì—­ì „ {reversals}íšŒ. ë°ì´í„° ì •ë ¬ í•„ìš”."
                        ),
                        details={"reversal_count": int(reversals)},
                    ))

        # 3. ìŠ¤í…Œì¼ ë°ì´í„° ê²€ì‚¬
        if "_pit_ingest_time" in df.columns and "_pit_event_time" in df.columns:
            latency = (
                df["_pit_ingest_time"] - df["_pit_event_time"]
            ).dt.total_seconds()
            stale_mask = latency > self.stale_threshold
            stale_count = stale_mask.sum()
            if stale_count > 0:
                violations.append(PITViolation(
                    violation_type="stale_data",
                    severity="warning",
                    description=(
                        f"ìŠ¤í…Œì¼ ë°ì´í„° {stale_count}í–‰ "
                        f"(ì§€ì—° >{self.stale_threshold}ì´ˆ)."
                    ),
                    details={
                        "count": int(stale_count),
                        "max_latency_s": round(float(latency.max()), 1),
                    },
                ))

        # 4. ì†ŒìŠ¤ ì¼ê´€ì„± ê²€ì‚¬
        if "_pit_source" in df.columns:
            sources = df["_pit_source"].unique()
            if len(sources) > 1:
                violations.append(PITViolation(
                    violation_type="source_mismatch",
                    severity="warning",
                    description=(
                        f"ë°ì´í„° ì†ŒìŠ¤ {len(sources)}ê°œ í˜¼í•©: "
                        f"{', '.join(str(s) for s in sources)}"
                    ),
                    details={"sources": list(str(s) for s in sources)},
                ))

        return violations


# â”€â”€ Source Registry â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class SourceRegistry:
    """ë°ì´í„° ì†ŒìŠ¤ ì´ë ¥ ê´€ë¦¬.

    ê° ì†ŒìŠ¤ì˜ ë§ˆì§€ë§‰ ì„±ê³µ/ì‹¤íŒ¨, í‰ê·  ì§€ì—°, ì‹ ë¢°ë„ë¥¼ ì¶”ì í•œë‹¤.
    """

    def __init__(self):
        self._sources: dict[str, dict] = {}

    def record_fetch(
        self,
        source: str,
        ticker: str,
        success: bool,
        latency_ms: float = 0.0,
        record_count: int = 0,
    ) -> None:
        """ë°ì´í„° ìˆ˜ì§‘ ê²°ê³¼ ê¸°ë¡."""
        key = f"{source}:{ticker}"
        if key not in self._sources:
            self._sources[key] = {
                "source": source,
                "ticker": ticker,
                "success_count": 0,
                "failure_count": 0,
                "total_latency_ms": 0.0,
                "last_success": None,
                "last_failure": None,
                "total_records": 0,
            }

        entry = self._sources[key]
        if success:
            entry["success_count"] += 1
            entry["total_latency_ms"] += latency_ms
            entry["last_success"] = datetime.now(KST).isoformat()
            entry["total_records"] += record_count
        else:
            entry["failure_count"] += 1
            entry["last_failure"] = datetime.now(KST).isoformat()

    def get_reliability(self, source: str, ticker: str = "") -> float:
        """ì†ŒìŠ¤ ì‹ ë¢°ë„ (0~1)."""
        key = f"{source}:{ticker}"
        entry = self._sources.get(key)
        if not entry:
            return 1.0  # ê¸°ë³¸ê°’
        total = entry["success_count"] + entry["failure_count"]
        if total == 0:
            return 1.0
        return entry["success_count"] / total

    def get_avg_latency_ms(self, source: str, ticker: str = "") -> float:
        """ì†ŒìŠ¤ í‰ê·  ì§€ì—° (ms)."""
        key = f"{source}:{ticker}"
        entry = self._sources.get(key)
        if not entry or entry["success_count"] == 0:
            return 0.0
        return entry["total_latency_ms"] / entry["success_count"]

    def get_summary(self) -> list[dict]:
        """ì „ì²´ ì†ŒìŠ¤ ìƒíƒœ ìš”ì•½."""
        summary = []
        for key, entry in self._sources.items():
            total = entry["success_count"] + entry["failure_count"]
            reliability = entry["success_count"] / total if total > 0 else 1.0
            avg_lat = (
                entry["total_latency_ms"] / entry["success_count"]
                if entry["success_count"] > 0 else 0.0
            )
            summary.append({
                "source": entry["source"],
                "ticker": entry["ticker"],
                "reliability": round(reliability, 3),
                "avg_latency_ms": round(avg_lat, 1),
                "total_fetches": total,
                "total_records": entry["total_records"],
                "last_success": entry["last_success"],
                "last_failure": entry["last_failure"],
            })
        return summary


# â”€â”€ ê¸€ë¡œë²Œ ì¸ìŠ¤í„´ìŠ¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

_registry = SourceRegistry()
_validator = PITValidator()
_asof = AsOfJoinEngine()


def get_registry() -> SourceRegistry:
    """ê¸€ë¡œë²Œ SourceRegistry ë°˜í™˜."""
    return _registry


def get_validator() -> PITValidator:
    """ê¸€ë¡œë²Œ PITValidator ë°˜í™˜."""
    return _validator


def get_asof_engine() -> AsOfJoinEngine:
    """ê¸€ë¡œë²Œ AsOfJoinEngine ë°˜í™˜."""
    return _asof


# â”€â”€ í…”ë ˆê·¸ë¨ í¬ë§· â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def format_pit_status(violations: list[PITViolation]) -> str:
    """PIT ê²€ì¦ ê²°ê³¼ë¥¼ í…”ë ˆê·¸ë¨ í¬ë§·."""
    if not violations:
        return "âœ… PIT ë¬´ê²°ì„±: ì´ìƒ ì—†ìŒ"

    lines = ["ğŸ” PIT ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦", "â”" * 25, ""]
    critical = [v for v in violations if v.severity == "critical"]
    warnings = [v for v in violations if v.severity == "warning"]

    if critical:
        lines.append("ğŸ”´ ì‹¬ê°")
        for v in critical:
            lines.append(f"  â€¢ {v.description}")

    if warnings:
        lines.append("ğŸŸ¡ ê²½ê³ ")
        for v in warnings:
            lines.append(f"  â€¢ {v.description}")

    return "\n".join(lines)


def format_source_summary(registry: SourceRegistry | None = None) -> str:
    """ì†ŒìŠ¤ ìƒíƒœ ìš”ì•½ì„ í…”ë ˆê·¸ë¨ í¬ë§·."""
    reg = registry or _registry
    summary = reg.get_summary()

    if not summary:
        return "ğŸ“¡ ë°ì´í„° ì†ŒìŠ¤: ë¯¸ë“±ë¡"

    lines = ["ğŸ“¡ ë°ì´í„° ì†ŒìŠ¤ ìƒíƒœ", "â”" * 25, ""]
    for s in summary:
        icon = "ğŸŸ¢" if s["reliability"] >= 0.9 else "ğŸŸ¡" if s["reliability"] >= 0.7 else "ğŸ”´"
        lines.append(
            f"  {icon} {s['source']}"
            f" | ì‹ ë¢°ë„ {s['reliability']:.0%}"
            f" | í‰ê· ì§€ì—° {s['avg_latency_ms']:.0f}ms"
            f" | ìˆ˜ì§‘ {s['total_fetches']}íšŒ"
        )

    return "\n".join(lines)
