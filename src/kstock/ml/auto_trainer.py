"""ML ìë™ ì¬í•™ìŠµ + ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ + ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ìµœì í™”.

FreqAI ìŠ¤íƒ€ì¼ì˜ ìë™ ì¬í•™ìŠµ ì‹œìŠ¤í…œ:
- ëª¨ë¸ ë“œë¦¬í”„íŠ¸ ê°ì§€ (ì˜ˆì¸¡ ì •í™•ë„ í•˜ë½ â†’ ìë™ ì¬í•™ìŠµ íŠ¸ë¦¬ê±°)
- ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ë™ì  ìµœì í™” (LGB/XGB/LSTM ë¹„ìœ¨ ìë™ ì¡°ì •)
- í•™ìŠµ íˆìŠ¤í† ë¦¬ ì¶”ì  + í…”ë ˆê·¸ë¨ ë¦¬í¬íŠ¸

ë¹„ìš©: CPU only, ì™¸ë¶€ API ì—†ìŒ.
"""
from __future__ import annotations

import json
import logging
import os
import time
from dataclasses import dataclass, field
from datetime import date, datetime, timedelta
from pathlib import Path
from typing import Any

import numpy as np

logger = logging.getLogger(__name__)

MODEL_DIR = Path(os.path.dirname(__file__)).parent.parent.parent / "models"
HISTORY_FILE = MODEL_DIR / "train_history.json"


# â”€â”€ Data Classes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@dataclass
class ModelPerformance:
    """ë‹¨ì¼ ëª¨ë¸ì˜ ì„±ëŠ¥ ì§€í‘œ."""
    model_name: str
    date: str
    accuracy: float = 0.0       # ì˜ˆì¸¡ ì •í™•ë„ (0~1)
    precision: float = 0.0      # ì–‘ì„± ì •ë°€ë„
    recall: float = 0.0         # ì¬í˜„ìœ¨
    auc: float = 0.5            # AUC-ROC
    sharpe_ratio: float = 0.0   # ì˜ˆì¸¡ ê¸°ë°˜ ë§¤ë§¤ì˜ Sharpe ratio
    predictions_count: int = 0
    correct_count: int = 0


@dataclass
class TrainHistory:
    """í•™ìŠµ íˆìŠ¤í† ë¦¬ ë‹¨ì¼ ë ˆì½”ë“œ."""
    train_date: str
    trigger: str       # "scheduled", "drift", "manual"
    samples: int
    train_auc: float
    val_auc: float
    lgb_weight: float
    xgb_weight: float
    lstm_weight: float
    duration_sec: float
    overfitting: bool = False


@dataclass
class DriftReport:
    """ëª¨ë¸ ë“œë¦¬í”„íŠ¸ ê°ì§€ ê²°ê³¼."""
    is_drifting: bool
    current_accuracy: float
    baseline_accuracy: float
    accuracy_drop_pct: float    # ì •í™•ë„ í•˜ë½ë¥  (%)
    days_since_train: int
    retrain_recommended: bool
    reason: str


@dataclass
class AutoTrainResult:
    """ìë™ ì¬í•™ìŠµ ì‹¤í–‰ ê²°ê³¼."""
    success: bool
    trigger: str
    samples: int = 0
    train_auc: float = 0.0
    val_auc: float = 0.0
    optimal_weights: tuple[float, float, float] = (0.35, 0.30, 0.35)
    duration_sec: float = 0.0
    message: str = ""


# â”€â”€ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class ModelMonitor:
    """ML ëª¨ë¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ + ë“œë¦¬í”„íŠ¸ ê°ì§€."""

    # ë“œë¦¬í”„íŠ¸ ê°ì§€ ì„ê³„ê°’
    ACCURACY_DROP_THRESHOLD = 0.10   # 10% ì´ìƒ ì •í™•ë„ í•˜ë½
    MIN_EVAL_SAMPLES = 20            # ìµœì†Œ í‰ê°€ ìƒ˜í”Œ ìˆ˜
    MAX_DAYS_WITHOUT_RETRAIN = 14    # ìµœëŒ€ ì¬í•™ìŠµ ì—†ì´ ìš´ì˜ ì¼ìˆ˜

    def __init__(self, db=None) -> None:
        self.db = db
        self._prediction_log: list[dict] = []  # ìµœê·¼ ì˜ˆì¸¡ ê¸°ë¡
        self._baseline_accuracy: float = 0.6   # ê¸°ì¤€ ì •í™•ë„

    def log_prediction(
        self,
        ticker: str,
        predicted_prob: float,
        actual_return_5d: float | None = None,
    ) -> None:
        """ì˜ˆì¸¡ ê²°ê³¼ ê¸°ë¡. 5ì¼ í›„ ì‹¤ì œ ìˆ˜ìµë¥ ë¡œ í‰ê°€."""
        entry = {
            "ticker": ticker,
            "date": date.today().isoformat(),
            "predicted_prob": predicted_prob,
            "predicted_label": "BUY" if predicted_prob >= 0.6 else "AVOID",
            "actual_return_5d": actual_return_5d,
            "correct": None,
        }

        if actual_return_5d is not None:
            actual_label = 1 if actual_return_5d > 0.03 else 0
            pred_label = 1 if predicted_prob >= 0.5 else 0
            entry["correct"] = actual_label == pred_label

        self._prediction_log.append(entry)
        # ìµœëŒ€ 500ê°œ ìœ ì§€
        if len(self._prediction_log) > 500:
            self._prediction_log = self._prediction_log[-500:]

    def evaluate_recent(self, days: int = 14) -> ModelPerformance:
        """ìµœê·¼ Nì¼ê°„ ì˜ˆì¸¡ ì„±ëŠ¥ í‰ê°€."""
        cutoff = (date.today() - timedelta(days=days)).isoformat()
        recent = [
            p for p in self._prediction_log
            if p["date"] >= cutoff and p["correct"] is not None
        ]

        perf = ModelPerformance(
            model_name="ensemble",
            date=date.today().isoformat(),
            predictions_count=len(recent),
        )

        if len(recent) < self.MIN_EVAL_SAMPLES:
            perf.accuracy = self._baseline_accuracy
            return perf

        correct = sum(1 for p in recent if p["correct"])
        perf.correct_count = correct
        perf.accuracy = correct / len(recent)

        # Precision: BUY ì˜ˆì¸¡ ì¤‘ ì‹¤ì œ +3% ë‹¬ì„±
        buy_preds = [p for p in recent if p["predicted_prob"] >= 0.5]
        if buy_preds:
            buy_correct = sum(1 for p in buy_preds if p.get("actual_return_5d", 0) > 0.03)
            perf.precision = buy_correct / len(buy_preds)

        # Recall: ì‹¤ì œ +3% ì¤‘ BUYë¡œ ì˜ˆì¸¡
        actual_buys = [p for p in recent if p.get("actual_return_5d", 0) > 0.03]
        if actual_buys:
            recalled = sum(1 for p in actual_buys if p["predicted_prob"] >= 0.5)
            perf.recall = recalled / len(actual_buys)

        return perf

    def detect_drift(self, last_train_date: date | None = None) -> DriftReport:
        """ëª¨ë¸ ë“œë¦¬í”„íŠ¸ ê°ì§€."""
        perf = self.evaluate_recent()
        days_since = (
            (date.today() - last_train_date).days
            if last_train_date else 999
        )

        accuracy_drop = self._baseline_accuracy - perf.accuracy
        drop_pct = (accuracy_drop / max(self._baseline_accuracy, 0.01)) * 100

        # ì¬í•™ìŠµ ì¶”ì²œ ì¡°ê±´
        retrain_reasons = []
        if accuracy_drop > self.ACCURACY_DROP_THRESHOLD:
            retrain_reasons.append(f"ì •í™•ë„ {drop_pct:.1f}% í•˜ë½")
        if days_since > self.MAX_DAYS_WITHOUT_RETRAIN:
            retrain_reasons.append(f"ìµœê·¼ í•™ìŠµ {days_since}ì¼ ì „")
        if perf.predictions_count < self.MIN_EVAL_SAMPLES:
            # ë°ì´í„° ë¶€ì¡± ì‹œì—ëŠ” ì‹œê°„ ê¸°ë°˜ ì¬í•™ìŠµë§Œ
            if days_since > self.MAX_DAYS_WITHOUT_RETRAIN:
                retrain_reasons.append("í‰ê°€ ë°ì´í„° ë¶€ì¡± + ì¥ê¸° ë¯¸í•™ìŠµ")

        is_drifting = len(retrain_reasons) > 0
        reason = " / ".join(retrain_reasons) if retrain_reasons else "ì •ìƒ ë²”ìœ„"

        return DriftReport(
            is_drifting=is_drifting,
            current_accuracy=round(perf.accuracy, 4),
            baseline_accuracy=round(self._baseline_accuracy, 4),
            accuracy_drop_pct=round(drop_pct, 1),
            days_since_train=days_since,
            retrain_recommended=is_drifting,
            reason=reason,
        )

    def update_baseline(self, new_accuracy: float) -> None:
        """í•™ìŠµ í›„ ê¸°ì¤€ ì •í™•ë„ ê°±ì‹ ."""
        if new_accuracy > 0.5:
            self._baseline_accuracy = new_accuracy


# â”€â”€ ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ìµœì í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def optimize_ensemble_weights(
    lgb_probs: list[float],
    xgb_probs: list[float],
    lstm_probs: list[float],
    actuals: list[int],
    grid_steps: int = 20,
) -> tuple[float, float, float]:
    """Grid searchë¡œ ìµœì  ì•™ìƒë¸” ê°€ì¤‘ì¹˜ íƒìƒ‰.

    Args:
        lgb_probs: LightGBM ì˜ˆì¸¡ í™•ë¥  ë¦¬ìŠ¤íŠ¸
        xgb_probs: XGBoost ì˜ˆì¸¡ í™•ë¥  ë¦¬ìŠ¤íŠ¸
        lstm_probs: LSTM ì˜ˆì¸¡ í™•ë¥  ë¦¬ìŠ¤íŠ¸
        actuals: ì‹¤ì œ ë¼ë²¨ (0/1) ë¦¬ìŠ¤íŠ¸
        grid_steps: ê·¸ë¦¬ë“œ íƒìƒ‰ ë‹¨ìœ„ ìˆ˜

    Returns:
        (w_lgb, w_xgb, w_lstm) ìµœì  ê°€ì¤‘ì¹˜ íŠœí”Œ
    """
    if not lgb_probs or len(lgb_probs) != len(actuals):
        return (0.35, 0.30, 0.35)

    lgb_arr = np.array(lgb_probs)
    xgb_arr = np.array(xgb_probs) if xgb_probs else lgb_arr * 0.95
    lstm_arr = np.array(lstm_probs) if lstm_probs else np.full(len(lgb_arr), 0.5)
    actual_arr = np.array(actuals)

    best_score = -1.0
    best_weights = (0.35, 0.30, 0.35)

    step = 1.0 / grid_steps
    for w1_steps in range(1, grid_steps):
        w1 = w1_steps * step
        for w2_steps in range(1, grid_steps - w1_steps):
            w2 = w2_steps * step
            w3 = 1.0 - w1 - w2
            if w3 < step:
                continue

            combined = w1 * lgb_arr + w2 * xgb_arr + w3 * lstm_arr
            preds = (combined >= 0.5).astype(int)

            accuracy = np.mean(preds == actual_arr)

            # ë³´ì •: ì˜ˆì¸¡ ê¸°ë°˜ ì‹œë®¬ë ˆì´ì…˜ ìˆ˜ìµë¥  ê°€ì¤‘
            buy_mask = combined >= 0.6
            if buy_mask.any():
                # BUY ì‹ í˜¸ ì •ë°€ë„ ë³´ë„ˆìŠ¤
                buy_correct = np.mean(actual_arr[buy_mask] == 1) if buy_mask.sum() > 0 else 0
                score = accuracy * 0.6 + buy_correct * 0.4
            else:
                score = accuracy * 0.7

            if score > best_score:
                best_score = score
                best_weights = (round(w1, 2), round(w2, 2), round(w3, 2))

    logger.info(
        "Optimal ensemble weights: LGB=%.2f, XGB=%.2f, LSTM=%.2f (score=%.4f)",
        *best_weights, best_score,
    )
    return best_weights


# â”€â”€ ìë™ ì¬í•™ìŠµ ì—”ì§„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class AutoTrainer:
    """FreqAI ìŠ¤íƒ€ì¼ ìë™ ì¬í•™ìŠµ ì—”ì§„.

    ì£¼ìš” ê¸°ëŠ¥:
    1. ì£¼ê°„ ì •ê¸° ì¬í•™ìŠµ (ì¼ìš”ì¼ 03:00)
    2. ë“œë¦¬í”„íŠ¸ ê°ì§€ â†’ ë¹„ì •ê¸° ì¬í•™ìŠµ
    3. ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ë™ì  ìµœì í™”
    4. í•™ìŠµ íˆìŠ¤í† ë¦¬ ì¶”ì 
    """

    def __init__(self, db=None, yf_client=None) -> None:
        self.db = db
        self.yf_client = yf_client
        self.monitor = ModelMonitor(db)
        self._last_train_date: date | None = None
        self._current_weights = (0.35, 0.30, 0.35)
        self._history: list[TrainHistory] = []
        self._load_history()

    def _load_history(self) -> None:
        """ë””ìŠ¤í¬ì—ì„œ í•™ìŠµ íˆìŠ¤í† ë¦¬ ë¡œë“œ."""
        try:
            if HISTORY_FILE.exists():
                data = json.loads(HISTORY_FILE.read_text("utf-8"))
                self._history = [TrainHistory(**h) for h in data.get("history", [])]
                w = data.get("current_weights", [0.35, 0.30, 0.35])
                self._current_weights = tuple(w)
                last = data.get("last_train_date")
                if last:
                    self._last_train_date = date.fromisoformat(last)
                logger.debug(
                    "Loaded %d train history records, weights=%s",
                    len(self._history), self._current_weights,
                )
        except Exception as e:
            logger.debug("Train history load failed: %s", e)

    def _save_history(self) -> None:
        """í•™ìŠµ íˆìŠ¤í† ë¦¬ ë””ìŠ¤í¬ ì €ì¥."""
        try:
            MODEL_DIR.mkdir(parents=True, exist_ok=True)
            data = {
                "history": [
                    {
                        "train_date": h.train_date,
                        "trigger": h.trigger,
                        "samples": h.samples,
                        "train_auc": h.train_auc,
                        "val_auc": h.val_auc,
                        "lgb_weight": h.lgb_weight,
                        "xgb_weight": h.xgb_weight,
                        "lstm_weight": h.lstm_weight,
                        "duration_sec": h.duration_sec,
                        "overfitting": h.overfitting,
                    }
                    for h in self._history[-50:]  # ìµœê·¼ 50íšŒë§Œ ìœ ì§€
                ],
                "current_weights": list(self._current_weights),
                "last_train_date": self._last_train_date.isoformat() if self._last_train_date else None,
            }
            HISTORY_FILE.write_text(json.dumps(data, ensure_ascii=False, indent=2), "utf-8")
        except Exception as e:
            logger.debug("Train history save failed: %s", e)

    @property
    def current_weights(self) -> tuple[float, float, float]:
        """í˜„ì¬ ì•™ìƒë¸” ê°€ì¤‘ì¹˜."""
        return self._current_weights

    def should_retrain(self) -> DriftReport:
        """ì¬í•™ìŠµ í•„ìš” ì—¬ë¶€ í™•ì¸ (ë“œë¦¬í”„íŠ¸ ê°ì§€)."""
        return self.monitor.detect_drift(self._last_train_date)

    async def run_auto_train(
        self,
        trigger: str = "scheduled",
        training_data: list[dict] | None = None,
    ) -> AutoTrainResult:
        """ìë™ ì¬í•™ìŠµ ì‹¤í–‰.

        Args:
            trigger: "scheduled" (ì •ê¸°), "drift" (ë“œë¦¬í”„íŠ¸), "manual" (ìˆ˜ë™)
            training_data: í•™ìŠµ ë°ì´í„°. Noneì´ë©´ DBì—ì„œ ìˆ˜ì§‘.

        Returns:
            AutoTrainResult
        """
        start_time = time.time()

        try:
            from kstock.ml.predictor import build_training_data, train_model
            from kstock.ml.lstm_predictor import (
                build_sequences, train_lstm, save_lstm_model,
            )

            # 1. í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘
            if training_data is None:
                training_data = await self._collect_training_data()

            if not training_data or len(training_data) < 50:
                return AutoTrainResult(
                    success=False,
                    trigger=trigger,
                    samples=len(training_data) if training_data else 0,
                    message=f"í•™ìŠµ ë°ì´í„° ë¶€ì¡±: {len(training_data) if training_data else 0}ê°œ (<50)",
                )

            # 2. Feature matrix êµ¬ì¶•
            X, y = build_training_data(training_data)
            if len(X) == 0:
                return AutoTrainResult(
                    success=False, trigger=trigger,
                    message="Feature matrix ìƒì„± ì‹¤íŒ¨",
                )

            # 3. LightGBM + XGBoost í•™ìŠµ
            result = train_model(X, y, n_trials=20)
            lgb_model = result["model"]["lgb"]
            xgb_model = result["model"]["xgb"]
            metrics = result.get("metrics", {})

            # 4. LSTM í•™ìŠµ (ì‹œí€€ìŠ¤ ë°ì´í„° ìˆëŠ” ê²½ìš°)
            lstm_result_auc = 0.5
            try:
                X_seq, y_seq = build_sequences(X, y)
                if len(X_seq) > 30:
                    lstm_model, scaler_mean, scaler_std, lstm_train_result = train_lstm(
                        X_seq, y_seq, epochs=30, patience=7
                    )
                    if lstm_model is not None:
                        save_lstm_model(lstm_model, scaler_mean, scaler_std)
                        lstm_result_auc = lstm_train_result.val_auc
            except Exception as e:
                logger.debug("LSTM training skipped: %s", e)

            # 5. ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ìµœì í™”
            optimal_weights = self._current_weights
            try:
                if lgb_model is not None:
                    lgb_probs = lgb_model.predict(X).tolist()
                    xgb_probs = (
                        xgb_model.predict_proba(X)[:, 1].tolist()
                        if xgb_model is not None else []
                    )
                    lstm_probs = []  # LSTMì€ ì‹œí€€ìŠ¤ ê¸°ë°˜ì´ë¯€ë¡œ ë³„ë„ ì²˜ë¦¬ í•„ìš”
                    optimal_weights = optimize_ensemble_weights(
                        lgb_probs, xgb_probs, lstm_probs, y.tolist()
                    )
                    self._current_weights = optimal_weights
            except Exception as e:
                logger.debug("Weight optimization skipped: %s", e)

            # 6. ì„±ëŠ¥ ê¸°ë¡
            train_auc = metrics.get("train_auc", 0)
            val_auc = metrics.get("test_auc", 0)
            overfitting = metrics.get("overfitting_warning", False)
            duration = time.time() - start_time

            self._last_train_date = date.today()
            self.monitor.update_baseline(val_auc if val_auc > 0.5 else 0.6)

            history = TrainHistory(
                train_date=date.today().isoformat(),
                trigger=trigger,
                samples=len(X),
                train_auc=round(train_auc, 4),
                val_auc=round(val_auc, 4),
                lgb_weight=optimal_weights[0],
                xgb_weight=optimal_weights[1],
                lstm_weight=optimal_weights[2],
                duration_sec=round(duration, 1),
                overfitting=overfitting,
            )
            self._history.append(history)
            self._save_history()

            # DBì— ì„±ëŠ¥ ê¸°ë¡
            if self.db:
                try:
                    self.db.add_ml_performance(
                        model_name="ensemble_v4",
                        train_date=date.today().isoformat(),
                        train_auc=train_auc,
                        val_auc=val_auc,
                        samples=len(X),
                        weights=json.dumps(list(optimal_weights)),
                    )
                except Exception:
                    pass

            message = (
                f"âœ… ML ì¬í•™ìŠµ ì™„ë£Œ ({trigger})\n"
                f"  ìƒ˜í”Œ: {len(X):,}ê°œ\n"
                f"  Train AUC: {train_auc:.4f}\n"
                f"  Val AUC: {val_auc:.4f}\n"
                f"  LSTM AUC: {lstm_result_auc:.4f}\n"
                f"  ê°€ì¤‘ì¹˜: LGB={optimal_weights[0]:.2f} / "
                f"XGB={optimal_weights[1]:.2f} / LSTM={optimal_weights[2]:.2f}\n"
                f"  ì†Œìš”: {duration:.1f}ì´ˆ"
            )
            if overfitting:
                message += "\n  âš ï¸ ê³¼ì í•© ê°ì§€ â€” ê·œì œ ê°•í™” í•„ìš”"

            logger.info(message)

            return AutoTrainResult(
                success=True,
                trigger=trigger,
                samples=len(X),
                train_auc=round(train_auc, 4),
                val_auc=round(val_auc, 4),
                optimal_weights=optimal_weights,
                duration_sec=round(duration, 1),
                message=message,
            )

        except Exception as e:
            duration = time.time() - start_time
            logger.error("Auto-train failed: %s", e)
            return AutoTrainResult(
                success=False,
                trigger=trigger,
                duration_sec=round(duration, 1),
                message=f"âŒ ML ì¬í•™ìŠµ ì‹¤íŒ¨: {e}",
            )

    async def _collect_training_data(self) -> list[dict]:
        """DB + yfinanceì—ì„œ í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘.

        ê¸°ì¡´ predictor.build_features()ë¡œ feature dict êµ¬ì„± í›„
        5ì˜ì—…ì¼ í›„ ìˆ˜ìµë¥  ê¸°ì¤€ target ë¼ë²¨ë§.
        """
        if not self.db:
            return []

        # DBì—ì„œ ì¶”ì²œ ì´ë ¥ ê¸°ë°˜ í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘
        training_data = []
        try:
            # recommendation_resultsì—ì„œ ì˜ˆì¸¡-ì‹¤ì œ ëŒ€ë¹„
            recs = self.db.get_recommendation_results(limit=500)
            for r in recs:
                features = r.get("features", {})
                if isinstance(features, str):
                    try:
                        features = json.loads(features)
                    except json.JSONDecodeError:
                        continue

                if not features or len(features) < 10:
                    continue

                actual_return = r.get("actual_return_pct", 0) or 0
                target = 1 if actual_return > 3.0 else 0
                features["target"] = target
                training_data.append(features)

        except Exception as e:
            logger.debug("Training data collection: %s", e)

        # ë°ì´í„° ë¶€ì¡± ì‹œ í•©ì„± ë°ì´í„°ë¡œ ë³´ì¶© (ìµœì†Œ 50ê°œ)
        if len(training_data) < 50:
            logger.debug(
                "Training data insufficient (%d), generating synthetic",
                len(training_data),
            )
            training_data.extend(_generate_synthetic_training_data(
                50 - len(training_data)
            ))

        return training_data

    def format_train_report(self) -> str:
        """ìµœê·¼ í•™ìŠµ íˆìŠ¤í† ë¦¬ í…”ë ˆê·¸ë¨ í¬ë§·."""
        if not self._history:
            return "ğŸ“Š ML í•™ìŠµ ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤."

        recent = self._history[-5:]  # ìµœê·¼ 5íšŒ
        lines = [
            "ğŸ¤– ML ëª¨ë¸ ìƒíƒœ",
            "â”" * 22,
            "",
        ]

        if self._last_train_date:
            days_ago = (date.today() - self._last_train_date).days
            lines.append(f"ğŸ“… ìµœê·¼ í•™ìŠµ: {self._last_train_date} ({days_ago}ì¼ ì „)")
        lines.append(
            f"âš–ï¸ ê°€ì¤‘ì¹˜: LGB={self._current_weights[0]:.2f} / "
            f"XGB={self._current_weights[1]:.2f} / LSTM={self._current_weights[2]:.2f}"
        )
        lines.append("")

        for h in reversed(recent):
            icon = "âš ï¸" if h.overfitting else "âœ…"
            lines.append(
                f"{icon} {h.train_date} ({h.trigger}): "
                f"AUC {h.val_auc:.3f} | {h.samples}ê°œ | {h.duration_sec:.0f}s"
            )

        # ë“œë¦¬í”„íŠ¸ ìƒíƒœ
        drift = self.should_retrain()
        if drift.retrain_recommended:
            lines.extend(["", f"âš ï¸ ì¬í•™ìŠµ ê¶Œì¥: {drift.reason}"])

        return "\n".join(lines)


# â”€â”€ í•©ì„± í•™ìŠµ ë°ì´í„° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _generate_synthetic_training_data(n: int) -> list[dict]:
    """í•™ìŠµ ë°ì´í„° ë¶€ì¡± ì‹œ í•©ì„± ë°ì´í„° ìƒì„±.

    ì‹¤ì œ ë°ì´í„°ë¥¼ ëŒ€ì²´í•  ìˆ˜ëŠ” ì—†ì§€ë§Œ, ëª¨ë¸ ì´ˆê¸° í•™ìŠµì— ì‚¬ìš©.
    """
    from kstock.ml.predictor import FEATURE_NAMES

    rng = np.random.default_rng(42)
    data = []

    for _ in range(n):
        features = {}
        for fname in FEATURE_NAMES:
            if fname == "rsi":
                features[fname] = rng.uniform(20, 80)
            elif fname == "vix":
                features[fname] = rng.uniform(12, 35)
            elif fname == "per":
                features[fname] = rng.uniform(5, 50)
            elif fname in ("golden_cross", "dead_cross", "bb_squeeze", "mtf_aligned"):
                features[fname] = float(rng.random() > 0.7)
            elif fname in ("volume_ratio",):
                features[fname] = rng.uniform(0.5, 3.0)
            elif fname == "usdkrw":
                features[fname] = rng.uniform(1200, 1500)
            else:
                features[fname] = rng.standard_normal()

        # íƒ€ê²Ÿ: RSI ë‚®ê³  volume_ratio ë†’ìœ¼ë©´ ìƒìŠ¹ í™•ë¥  ë†’ìŒ (ê¸°ë³¸ ê·œì¹™)
        buy_signal = (
            features.get("rsi", 50) < 40
            and features.get("volume_ratio", 1) > 1.5
            and features.get("vix", 20) < 25
        )
        features["target"] = 1 if buy_signal else (1 if rng.random() > 0.65 else 0)
        data.append(features)

    return data
